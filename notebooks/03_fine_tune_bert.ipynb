{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc02b9a",
   "metadata": {},
   "source": [
    "# AI-Generated Text Detection - Fine-tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e63f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to ../data/processed_hc3.csv with 85431 valid samples.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run preprocessing script to ensure dataset is ready\n",
    "!python3 ../scripts/data_preprocessing.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cef5d2",
   "metadata": {},
   "source": [
    "## Train BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26913452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.53.2\n",
      "Uninstalling transformers-4.53.2:\n",
      "  Successfully uninstalled transformers-4.53.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: requests in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: filelock in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ethanc/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.10)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.53.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall transformers -y\n",
    "%pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5127c739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethanc/Library/Python/3.10/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.53.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f597a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python not found\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22bb5c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.executable: /usr/local/bin/python3\n",
      "pip version: pip 22.0.4 from /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Name: transformers\n",
      "Version: 4.53.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /Users/ethanc/Library/Python/3.10/lib/python/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "\n",
      "transformers version: 4.53.2\n",
      "transformers loaded from: /Users/ethanc/Library/Python/3.10/lib/python/site-packages/transformers/__init__.py\n",
      "TrainingArguments signature: (output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[str, dict, NoneType] = None, deepspeed: Union[str, dict, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n",
      "Transformers version (inside train_bert.py): 4.53.2\n",
      "Training on device: cpu\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ethanc/Desktop/Northeastern/CS5100/Final Project/notebooks/../scripts/train_bert.py\", line 108, in <module>\n",
      "    train_bert_model()\n",
      "  File \"/Users/ethanc/Desktop/Northeastern/CS5100/Final Project/notebooks/../scripts/train_bert.py\", line 63, in train_bert_model\n",
      "    training_args = TrainingArguments(\n",
      "TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\n"
     ]
    }
   ],
   "source": [
    "# diagnostics + run your train_bert script\n",
    "import sys, inspect, subprocess\n",
    "\n",
    "# 1) Which python executable are we using?\n",
    "print(\"sys.executable:\", sys.executable)\n",
    "\n",
    "# 2) pip version & show transformers\n",
    "print(\"pip version:\", subprocess.check_output([sys.executable, \"-m\", \"pip\", \"--version\"], text=True).strip())\n",
    "print(subprocess.check_output([sys.executable, \"-m\", \"pip\", \"show\", \"transformers\"], text=True))\n",
    "\n",
    "# 3) transformers version & location\n",
    "import transformers\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "print(\"transformers loaded from:\", transformers.__file__)\n",
    "\n",
    "# 4) Inspect TrainingArguments signature\n",
    "print(\"TrainingArguments signature:\", inspect.signature(transformers.TrainingArguments))\n",
    "\n",
    "# 5) Finally, invoke your script with the same interpreter\n",
    "!{sys.executable} ../scripts/train_bert.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f494571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(85056) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.53.2\n",
      "Debug mode enabled: 200 training samples, 50 validation samples\n",
      "tokenizer_config.json: 100%|█████████████████| 48.0/48.0 [00:00<00:00, 46.8kB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 2.43MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 10.0MB/s]\n",
      "config.json: 100%|█████████████████████████████| 483/483 [00:00<00:00, 2.14MB/s]\n",
      "Training on device: mps\n",
      "model.safetensors: 100%|█████████████████████| 268M/268M [01:51<00:00, 2.41MB/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]/Users/ethanc/Library/Python/3.10/lib/python/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "{'loss': 0.6054, 'grad_norm': 2.461533784866333, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5324, 'grad_norm': 2.9644126892089844, 'learning_rate': 7.2e-06, 'epoch': 0.8}\n",
      "100%|███████████████████████████████████████████| 25/25 [00:06<00:00,  5.93it/s]\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 43%|███████████████████▎                         | 3/7 [00:00<00:00, 22.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.534895658493042, 'eval_accuracy': 0.72, 'eval_f1': 0.0, 'eval_runtime': 0.8623, 'eval_samples_per_second': 57.985, 'eval_steps_per_second': 8.118, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████████| 25/25 [00:06<00:00,  5.93it/s]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 25.33it/s]\u001b[A\n",
      "{'train_runtime': 9.0901, 'train_samples_per_second': 22.002, 'train_steps_per_second': 2.75, 'train_loss': 0.557952938079834, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████████| 25/25 [00:09<00:00,  2.75it/s]\n",
      "DistilBERT model saved to ../models/bert_model\n"
     ]
    }
   ],
   "source": [
    "# 1) Ensure accelerate is installed and up to date for Trainer compatibility\n",
    "# %pip install --upgrade \"accelerate>=0.26.0\"\n",
    "\n",
    "# 2) (Optional) Patch train_bert.py for eval_strategy if needed\n",
    "import fileinput\n",
    "\n",
    "for line in fileinput.input('../scripts/train_bert.py', inplace=True, backup='.bak'):\n",
    "    print(line.replace('evaluation_strategy=', 'eval_strategy='), end='')\n",
    "\n",
    "# 3) Re-invoke your script in the same environment\n",
    "import sys\n",
    "!{sys.executable} ../scripts/train_bert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec34c3",
   "metadata": {},
   "source": [
    "## Load and Evaluate BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01faeb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/bert_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_dir)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Example evaluation on a sample text\u001b[39;00m\n\u001b[1;32m     10\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example sentence to check if the model thinks it is AI or human.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/transformers/modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/transformers/modeling_utils.py:4839\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4830\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4832\u001b[0m     (\n\u001b[1;32m   4833\u001b[0m         model,\n\u001b[1;32m   4834\u001b[0m         missing_keys,\n\u001b[1;32m   4835\u001b[0m         unexpected_keys,\n\u001b[1;32m   4836\u001b[0m         mismatched_keys,\n\u001b[1;32m   4837\u001b[0m         offload_index,\n\u001b[1;32m   4838\u001b[0m         error_msgs,\n\u001b[0;32m-> 4839\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4845\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4857\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4858\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/transformers/modeling_utils.py:5183\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5179\u001b[0m     base_model_expected_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model_to_load\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   5180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   5181\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m task_specific_expected_keys \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m base_model_expected_keys \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m checkpoint_keys\n\u001b[1;32m   5182\u001b[0m     ):\n\u001b[0;32m-> 5183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5184\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe state dictionary of the model you are trying to load is corrupted. Are you sure it was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperly saved?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5186\u001b[0m         )\n\u001b[1;32m   5188\u001b[0m \u001b[38;5;66;03m# Get reverse key mapping\u001b[39;00m\n\u001b[1;32m   5189\u001b[0m reverse_key_renaming_mapping \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m key_renaming_mapping\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mValueError\u001b[0m: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_dir = '../models/bert_model'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "# Example evaluation on a sample text\n",
    "sample_text = \"This is an example sentence to check if the model thinks it is AI or human.\"\n",
    "inputs = tokenizer(sample_text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "print(f\"Sample text prediction: {'AI-generated' if prediction == 1 else 'Human-written'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
